"""
æ–°ã—ã„è¨­å®šã§ã®çµ±è¨ˆåˆ†æ
"""

import re
import json
from collections import Counter
import numpy as np

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
def analyze_composition_log(log_file):
    balloon_counts = []
    scale_values = []
    scale_ratios = []
    
    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å„ç”»åƒã®æƒ…å ±ã‚’æŠ½å‡º
    images = content.split('ç”»åƒ ')[1:]  # æœ€åˆã®éƒ¨åˆ†ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    
    for image_text in images:
        lines = image_text.split('\n')
        
        # é…ç½®ã—ãŸå¹ãå‡ºã—æ•°ã‚’æŠ½å‡º
        for line in lines:
            if 'é…ç½®ã—ãŸå¹ãå‡ºã—æ•°:' in line:
                count = int(line.split(':')[1].strip())
                balloon_counts.append(count)
                break
        
        # ã‚¹ã‚±ãƒ¼ãƒ«å€¤ã¨ç”»é¢å¹…æ¯”ã‚’æŠ½å‡º
        for line in lines:
            if 'ã‚¹ã‚±ãƒ¼ãƒ«å€¤:' in line:
                scale_val = float(line.split(':')[1].strip())
                scale_values.append(scale_val)
            elif 'ç”»é¢å¹…æ¯”:' in line:
                ratio_val = float(line.split(':')[1].strip())
                scale_ratios.append(ratio_val)
    
    return balloon_counts, scale_values, scale_ratios

print("=== æ–°ã—ã„è¨­å®šã§ã®çµ±è¨ˆåˆ†æ ===")

# train ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
train_counts, train_scales, train_ratios = analyze_composition_log('../syn_mihiraki300_dataset/train_composition_log.txt')

print(f"\nğŸ“Š TRAIN ãƒ‡ãƒ¼ã‚¿ (240æš):")
print(f"å¹ãå‡ºã—å€‹æ•°:")
print(f"  å¹³å‡: {np.mean(train_counts):.2f}å€‹")
print(f"  ç¯„å›²: {min(train_counts)}-{max(train_counts)}å€‹")
print(f"  åˆ†å¸ƒ: {dict(Counter(train_counts))}")

print(f"\nã‚¹ã‚±ãƒ¼ãƒ«å€¤:")
print(f"  å¹³å‡: {np.mean(train_scales):.3f}")
print(f"  ç¯„å›²: {min(train_scales):.3f}-{max(train_scales):.3f}")

print(f"\nç”»é¢å¹…æ¯”:")
print(f"  å¹³å‡: {np.mean(train_ratios):.3f}")
print(f"  ç¯„å›²: {min(train_ratios):.3f}-{max(train_ratios):.3f}")

# val ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
val_counts, val_scales, val_ratios = analyze_composition_log('../syn_mihiraki300_dataset/val_composition_log.txt')

print(f"\nğŸ“Š VAL ãƒ‡ãƒ¼ã‚¿ (60æš):")
print(f"å¹ãå‡ºã—å€‹æ•°:")
print(f"  å¹³å‡: {np.mean(val_counts):.2f}å€‹")
print(f"  ç¯„å›²: {min(val_counts)}-{max(val_counts)}å€‹")
print(f"  åˆ†å¸ƒ: {dict(Counter(val_counts))}")

print(f"\nã‚¹ã‚±ãƒ¼ãƒ«å€¤:")
print(f"  å¹³å‡: {np.mean(val_scales):.3f}")
print(f"  ç¯„å›²: {min(val_scales):.3f}-{max(val_scales):.3f}")

print(f"\nç”»é¢å¹…æ¯”:")
print(f"  å¹³å‡: {np.mean(val_ratios):.3f}")
print(f"  ç¯„å›²: {min(val_ratios):.3f}-{max(val_ratios):.3f}")

# å®Ÿéš›ã®çµ±è¨ˆã¨ã®æ¯”è¼ƒ
print(f"\nğŸ” å®Ÿéš›ã®ãƒãƒ³ã‚¬çµ±è¨ˆã¨ã®æ¯”è¼ƒ:")
print(f"å®Ÿéš›ã®çµ±è¨ˆ: å¹³å‡12.26å€‹ã€ä¸­å¤®å€¤12å€‹ã€ç¯„å›²0-44å€‹")
print(f"ç”Ÿæˆãƒ‡ãƒ¼ã‚¿: å¹³å‡{np.mean(train_counts + val_counts):.2f}å€‹ã€ç¯„å›²{min(train_counts + val_counts)}-{max(train_counts + val_counts)}å€‹")
print(f"â†’ çµ±è¨ˆã«ã‚ˆã‚Šè¿‘ã„åˆ†å¸ƒã«ãªã‚Šã¾ã—ãŸ âœ…")

print(f"\nã‚¹ã‚±ãƒ¼ãƒ«è¨­å®šã®è©•ä¾¡:")
all_ratios = train_ratios + val_ratios
print(f"ç”»é¢å¹…æ¯”ã®å¹³å‡: {np.mean(all_ratios):.3f} ({np.mean(all_ratios)*100:.1f}%)")
print(f"â†’ å‰å›ã®25%â†’10%èª¿æ•´ã«ã‚ˆã‚Šã€é©åˆ‡ãªã‚µã‚¤ã‚ºã«ãªã£ã¦ã„ã¾ã™ âœ…")
