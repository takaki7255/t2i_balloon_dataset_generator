"""
ã‚ªãƒãƒãƒˆãƒšã‚’ã‚³ãƒå†…ã«é…ç½®ã™ã‚‹åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

create_corner_aligned_dataset.pyã®ãƒ‘ãƒãƒ«æ¤œå‡ºæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã€
ã‚ªãƒãƒãƒˆãƒšã‚’ã‚³ãƒã®å†…å´ã«ãƒ©ãƒ³ãƒ€ãƒ ã«é…ç½®ã™ã‚‹
"""

import cv2
import numpy as np
from pathlib import Path
import os
import random
import shutil
from tqdm import tqdm
import json
from datetime import datetime
import argparse
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass


@dataclass
class Point:
    """åº§æ¨™ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    x: int
    y: int
    
    def __iter__(self):
        return iter((self.x, self.y))


def regions_overlap(region1: tuple, region2: tuple) -> bool:
    """2ã¤ã®é ˜åŸŸãŒé‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    x1_min, y1_min, x1_max, y1_max = region1
    x2_min, y2_min, x2_max, y2_max = region2
    
    return not (x1_max <= x2_min or x2_max <= x1_min or y1_max <= y2_min or y2_max <= y1_min)


def calculate_overlap_area(region1: tuple, region2: tuple) -> int:
    """2ã¤ã®é ˜åŸŸã®é‡è¤‡é¢ç©ã‚’è¨ˆç®—"""
    x1_min, y1_min, x1_max, y1_max = region1
    x2_min, y2_min, x2_max, y2_max = region2
    
    overlap_x_min = max(x1_min, x2_min)
    overlap_y_min = max(y1_min, y2_min)
    overlap_x_max = min(x1_max, x2_max)
    overlap_y_max = min(y1_max, y2_max)
    
    if overlap_x_max <= overlap_x_min or overlap_y_max <= overlap_y_min:
        return 0
    
    return (overlap_x_max - overlap_x_min) * (overlap_y_max - overlap_y_min)


def get_mask_bbox(mask):
    """ãƒã‚¹ã‚¯ã®éã‚¼ãƒ­é ˜åŸŸã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—"""
    coords = np.column_stack(np.where(mask > 0))
    
    if len(coords) == 0:
        return 0, 0, mask.shape[1], mask.shape[0]
    
    y_min, x_min = coords.min(axis=0)
    y_max, x_max = coords.max(axis=0)
    
    return x_min, y_min, x_max - x_min + 1, y_max - y_min + 1


def crop_onomatopoeia_and_mask(onomatopoeia, mask):
    """ãƒã‚¹ã‚¯ã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ã‚ªãƒãƒãƒˆãƒšç”»åƒã¨ãƒã‚¹ã‚¯ã‚’ã‚¯ãƒ­ãƒƒãƒ—"""
    x, y, w, h = get_mask_bbox(mask)
    
    cropped_onomatopoeia = onomatopoeia[y:y+h, x:x+w]
    cropped_mask = mask[y:y+h, x:x+w]
    
    return cropped_onomatopoeia, cropped_mask, (x, y, w, h)


def detect_panels_simple(image: np.ndarray, 
                        area_ratio_threshold: float = 0.85,
                        min_area: int = 10000) -> List[Tuple[np.ndarray, Tuple[int, int, int, int]]]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªäºŒå€¤åŒ–ãƒ»è¼ªéƒ­æŠ½å‡ºã«ã‚ˆã‚‹ã‚³ãƒæ¤œå‡º
    
    Args:
        image: å…¥åŠ›ç”»åƒï¼ˆã‚«ãƒ©ãƒ¼ï¼‰
        area_ratio_threshold: è¼ªéƒ­é¢ç©/ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹é¢ç©ã®é–¾å€¤
        min_area: æœ€å°ã‚³ãƒé¢ç©
    
    Returns:
        List[(panel_mask, bbox)]: ãƒ‘ãƒãƒ«ãƒã‚¹ã‚¯ã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # äºŒå€¤åŒ–ï¼ˆOtsuã®è‡ªå‹•é–¾å€¤ï¼‰
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    # ãƒã‚¤ã‚ºé™¤å»ï¼ˆãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼æ¼”ç®—ï¼‰
    kernel = np.ones((3, 3), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)
    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)
    
    # è¼ªéƒ­æ¤œå‡º
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    panels = []
    
    for contour in contours:
        # è¼ªéƒ­ã®é¢ç©ã‚’è¨ˆç®—
        contour_area = cv2.contourArea(contour)
        
        # å°ã•ã™ãã‚‹è¼ªéƒ­ã¯ç„¡è¦–
        if contour_area < min_area:
            continue
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—
        x, y, w, h = cv2.boundingRect(contour)
        bbox_area = w * h
        
        # é¢ç©æ¯”ã‚’è¨ˆç®—
        area_ratio = contour_area / bbox_area if bbox_area > 0 else 0
        
        # é¢ç©æ¯”ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆã€ã‚³ãƒã¨ã—ã¦èªè­˜
        if area_ratio >= area_ratio_threshold:
            # ãƒ‘ãƒãƒ«ãƒã‚¹ã‚¯ã‚’ä½œæˆ
            panel_mask = np.zeros((h, w), dtype=np.uint8)
            
            # è¼ªéƒ­ã‚’ç›¸å¯¾åº§æ¨™ã«å¤‰æ›ã—ã¦ãƒã‚¹ã‚¯ã«æç”»
            contour_relative = contour - np.array([x, y])
            cv2.drawContours(panel_mask, [contour_relative], -1, 255, -1)
            
            panels.append((panel_mask, (x, y, w, h)))
    
    return panels


def sample_scale(bg_w: int, cfg: dict) -> float:
    """çµ±è¨ˆæƒ…å ±ã«åŸºã¥ã„ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰"""
    mode = cfg.get("SCALE_MODE", "uniform")
    if mode == "lognormal":
        clip_min, clip_max = cfg["SCALE_CLIP"]
        
        scale_stats = cfg.get("SCALE_STATS", {})
        target_median = scale_stats.get("bbox_median", 0.001488)
        sigma = 0.8
        
        mu = np.log(target_median)
        s = np.random.lognormal(mu, sigma)
        clipped = float(np.clip(s, clip_min, clip_max))
        return clipped
    else:
        return random.uniform(*cfg["SCALE_RANGE"])


def calculate_area_based_size(crop_w: int, crop_h: int, bg_w: int, bg_h: int, 
                             target_scale: float, mask: np.ndarray = None,
                             max_w_ratio: float = 0.25, 
                             max_h_ratio: float = 0.25) -> tuple:
    """é¢ç©ãƒ™ãƒ¼ã‚¹ã®ãƒªã‚µã‚¤ã‚ºã‚µã‚¤ã‚ºè¨ˆç®—"""
    bg_area = bg_w * bg_h
    target_area = bg_area * target_scale
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯” = é«˜ã•/å¹…ï¼ˆcreate_syn_dataset.pyã¨åŒã˜å®šç¾©ï¼‰
    aspect_ratio = crop_h / crop_w
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ãŸç†æƒ³ã‚µã‚¤ã‚º
    ideal_w = int(np.sqrt(target_area / aspect_ratio))
    ideal_h = int(np.sqrt(target_area * aspect_ratio))
    
    # æœ€å¤§ã‚µã‚¤ã‚ºåˆ¶é™
    max_w = int(bg_w * max_w_ratio)
    max_h = int(bg_h * max_h_ratio)
    
    # åˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´
    scale_by_w = max_w / ideal_w if ideal_w > max_w else 1.0
    scale_by_h = max_h / ideal_h if ideal_h > max_h else 1.0
    adjust_scale = min(scale_by_w, scale_by_h)
    
    new_w = int(ideal_w * adjust_scale)
    new_h = int(ideal_h * adjust_scale)
    
    # æœ€å°ã‚µã‚¤ã‚ºç¢ºä¿
    new_w = max(new_w, 10)
    new_h = max(new_h, 10)
    
    return new_w, new_h


def place_onomatopoeia_in_panel(panel_image: np.ndarray, panel_mask: np.ndarray,
                               panel_bbox: Tuple[int, int, int, int],
                               onomatopoeia: np.ndarray, onomatopoeia_mask: np.ndarray,
                               target_scale: float, bg_w: int, bg_h: int, cfg: dict) -> Tuple[bool, Optional[Dict]]:
    """
    ã‚ªãƒãƒãƒˆãƒšã‚’ã‚³ãƒå†…ã«é…ç½®
    
    Args:
        panel_image: ãƒ‘ãƒãƒ«ç”»åƒ
        panel_mask: ãƒ‘ãƒãƒ«ãƒã‚¹ã‚¯
        panel_bbox: ãƒ‘ãƒãƒ«ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ (x, y, w, h)
        onomatopoeia: ã‚ªãƒãƒãƒˆãƒšç”»åƒ
        onomatopoeia_mask: ã‚ªãƒãƒãƒˆãƒšãƒã‚¹ã‚¯
        target_scale: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆèƒŒæ™¯ç”»åƒå…¨ä½“ã«å¯¾ã™ã‚‹æ¯”ç‡ï¼‰
        bg_w: èƒŒæ™¯ç”»åƒã®å¹…
        bg_h: èƒŒæ™¯ç”»åƒã®é«˜ã•
        cfg: è¨­å®š
    
    Returns:
        (æˆåŠŸãƒ•ãƒ©ã‚°, é…ç½®æƒ…å ±)
    """
    panel_x, panel_y, panel_w, panel_h = panel_bbox
    
    # ã‚¯ãƒ­ãƒƒãƒ—
    cropped_ono, cropped_mask, _ = crop_onomatopoeia_and_mask(onomatopoeia, onomatopoeia_mask)
    
    if cropped_ono.size == 0:
        return False, None
    
    crop_h, crop_w = cropped_ono.shape[:2]
    
    # ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆèƒŒæ™¯ç”»åƒå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨ï¼‰
    new_w, new_h = calculate_area_based_size(
        crop_w, crop_h, bg_w, bg_h, target_scale,
        mask=cropped_mask,
        max_w_ratio=cfg.get("MAX_WIDTH_RATIO", 0.25),
        max_h_ratio=cfg.get("MAX_HEIGHT_RATIO", 0.25)
    )
    
    # ã‚³ãƒã‚ˆã‚Šå¤§ãããªã£ãŸã‚‰ã‚¹ã‚­ãƒƒãƒ—
    if new_w >= panel_w or new_h >= panel_h:
        return False, None
    
    # ãƒªã‚µã‚¤ã‚º
    ono_resized = cv2.resize(cropped_ono, (new_w, new_h))
    mask_resized = cv2.resize(cropped_mask, (new_w, new_h))
    
    # ã‚³ãƒå†…ã®ãƒ©ãƒ³ãƒ€ãƒ ä½ç½®ã«é…ç½®
    max_x = max(0, panel_w - new_w)
    max_y = max(0, panel_h - new_h)
    
    if max_x <= 0 or max_y <= 0:
        return False, None
    
    x_in_panel = random.randint(0, max_x)
    y_in_panel = random.randint(0, max_y)
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«åº§æ¨™ã«å¤‰æ›
    x_global = panel_x + x_in_panel
    y_global = panel_y + y_in_panel
    
    # ã‚³ãƒå†…ã«åã¾ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆãƒã‚¹ã‚¯é ˜åŸŸï¼‰
    mask_region = mask_resized > 0
    if not np.all(mask_region <= panel_mask[y_in_panel:y_in_panel+new_h, x_in_panel:x_in_panel+new_w]):
        return False, None
    
    placement_info = {
        "size": (new_w, new_h),
        "position": (x_global, y_global),
        "position_in_panel": (x_in_panel, y_in_panel),
        "scale": float(target_scale)
    }
    
    return True, placement_info


def composite_onomatopoeia_in_panels(background_path: str, onomatopoeia_mask_pairs: list,
                                    cfg: dict = None) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:
    """
    èƒŒæ™¯ç”»åƒã®ã‚³ãƒã‚’æ¤œå‡ºã—ã€å„ã‚³ãƒã«ã‚ªãƒãƒãƒˆãƒšã‚’é…ç½®
    """
    if cfg is None:
        cfg = {}
    
    # èƒŒæ™¯ç”»åƒèª­ã¿è¾¼ã¿
    background = cv2.imread(background_path, cv2.IMREAD_COLOR)
    if background is None:
        raise FileNotFoundError(f"èƒŒæ™¯ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {background_path}")
    
    bg_h, bg_w = background.shape[:2]
    result_img = background.copy()
    result_mask = np.zeros((bg_h, bg_w), dtype=np.uint8)
    
    # ãƒ‘ãƒãƒ«æ¤œå‡º
    panels = detect_panels_simple(background,
                                 area_ratio_threshold=cfg.get("PANEL_AREA_RATIO", 0.70),
                                 min_area=cfg.get("PANEL_MIN_AREA", 5000))
    
    if len(panels) == 0:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: èƒŒæ™¯ç”»åƒå…¨ä½“ã‚’ãƒ‘ãƒãƒ«ã¨ã—ã¦æ‰±ã†
        full_panel_mask = np.ones((bg_h, bg_w), dtype=np.uint8) * 255
        panels = [(full_panel_mask, (0, 0, bg_w, bg_h))]
    
    placements = []
    
    # å„ãƒ‘ãƒãƒ«ã«ã‚ªãƒãƒãƒˆãƒšã‚’é…ç½®
    for panel_mask, panel_bbox in panels:
        panel_x, panel_y, panel_w, panel_h = panel_bbox
        
        # ã“ã®ãƒ‘ãƒãƒ«ã«é…ç½®ã™ã‚‹ã‚ªãƒãƒãƒˆãƒšæ•°ã‚’æ±ºå®š
        num_onomatopoeia = random.randint(
            cfg.get("ONOMATOPOEIA_PER_PANEL", (1, 3))[0],
            cfg.get("ONOMATOPOEIA_PER_PANEL", (1, 3))[1]
        )
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
        selected_pairs = random.sample(onomatopoeia_mask_pairs, 
                                      min(num_onomatopoeia, len(onomatopoeia_mask_pairs)))
        
        placed_regions = []  # ã“ã®ãƒ‘ãƒãƒ«å†…ã§ã®é…ç½®æ¸ˆã¿é ˜åŸŸ
        
        for ono_path, mask_path in selected_pairs:
            onomatopoeia = cv2.imread(ono_path, cv2.IMREAD_COLOR)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            
            if onomatopoeia is None or mask is None:
                continue
            
            # ã‚¹ã‚±ãƒ¼ãƒ«æ±ºå®š
            ono_scale = sample_scale(bg_w, cfg)
            
            # é…ç½®è©¦è¡Œ
            max_attempts = cfg.get("MAX_ATTEMPTS", 50)
            placed = False
            
            for attempt in range(max_attempts):
                success, placement_info = place_onomatopoeia_in_panel(
                    panel_mask, panel_mask, panel_bbox,
                    onomatopoeia, mask, ono_scale, bg_w, bg_h, cfg
                )
                
                if not success:
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                x, y = placement_info["position"]
                w, h = placement_info["size"]
                new_region = (x, y, x + w, y + h)
                
                # ãƒ‘ãƒãƒ«å†…ã®ä»–ã®ã‚ªãƒãƒãƒˆãƒšã¨ã®é‡è¤‡ç¢ºèª
                has_overlap = False
                for placed_region in placed_regions:
                    if regions_overlap(new_region, placed_region):
                        has_overlap = True
                        break
                
                if not has_overlap:
                    # é…ç½®å®Ÿè¡Œ
                    x_in_panel, y_in_panel = placement_info["position_in_panel"]
                    w, h = placement_info["size"]
                    
                    # ç”»åƒé ˜åŸŸå–å¾—
                    ono_resized = cv2.resize(
                        cv2.imread(ono_path, cv2.IMREAD_COLOR),
                        (w, h)
                    )
                    mask_resized = cv2.resize(
                        cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE),
                        (w, h)
                    )
                    
                    # ã‚¯ãƒ­ãƒƒãƒ—ç‰ˆã‚’ä½¿ç”¨
                    ono_cropped, mask_cropped, _ = crop_onomatopoeia_and_mask(ono_resized, mask_resized)
                    
                    # ã‚°ãƒ­ãƒ¼ãƒãƒ«åº§æ¨™ã§é…ç½®
                    x_global = placement_info["position"][0]
                    y_global = placement_info["position"][1]
                    w_ono, h_ono = ono_cropped.shape[1], ono_cropped.shape[0]
                    
                    # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
                    mask_norm = mask_cropped.astype(np.float32) / 255.0
                    mask_3ch = cv2.merge([mask_norm, mask_norm, mask_norm])
                    
                    bg_region = result_img[y_global:y_global+h_ono, x_global:x_global+w_ono]
                    blended = ono_cropped.astype(np.float32) * mask_3ch + bg_region.astype(np.float32) * (1 - mask_3ch)
                    result_img[y_global:y_global+h_ono, x_global:x_global+w_ono] = blended.astype(np.uint8)
                    
                    # ãƒã‚¹ã‚¯åˆæˆ
                    result_mask[y_global:y_global+h_ono, x_global:x_global+w_ono] = np.maximum(
                        result_mask[y_global:y_global+h_ono, x_global:x_global+w_ono],
                        mask_cropped
                    )
                    
                    placed_regions.append(new_region)
                    placements.append({
                        "onomatopoeia_file": Path(ono_path).name,
                        "panel_bbox": panel_bbox,
                        "position": placement_info["position"],
                        "size": placement_info["size"],
                        "scale": placement_info["scale"]
                    })
                    
                    placed = True
                    break
    
    return result_img, result_mask, placements


def load_scale_stats(path: str) -> dict:
    """çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒ«çµ±è¨ˆæƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰"""
    stats = {
        "bbox_mean": None,
        "bbox_median": None,
        "bbox_std": None,
        "bbox_q25": None,
        "bbox_q75": None,
        "type": "bbox"
    }
    
    try:
        with open(path, encoding="utf-8") as f:
            content = f.read()
        
        in_bbox_section = False
        for line in content.split('\n'):
            if "Bounding Box Size Ratio Statistics:" in line:
                in_bbox_section = True
                continue
            if in_bbox_section:
                if "Area Statistics" in line:
                    break
                
                if "Mean:" in line:
                    stats["bbox_mean"] = float(line.split("Mean:")[1].strip())
                elif "Median:" in line:
                    stats["bbox_median"] = float(line.split("Median:")[1].strip())
                elif "Standard deviation:" in line:
                    stats["bbox_std"] = float(line.split("Standard deviation:")[1].strip())
                elif "25th percentile:" in line:
                    stats["bbox_q25"] = float(line.split("25th percentile:")[1].strip())
                elif "75th percentile:" in line:
                    stats["bbox_q75"] = float(line.split("75th percentile:")[1].strip())
        
        print(f"âœ“ ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã‚’èª­ã¿è¾¼ã¿: {path}")
        print(f"  ä¸­å¤®å€¤: {stats['bbox_median']:.6f}")
        return stats
    except Exception as e:
        print(f"çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return stats


def _json_default(o):
    """JSON serialization helper"""
    if isinstance(o, np.ndarray):
        return o.tolist()
    if isinstance(o, (np.integer, np.floating)):
        return o.item()
    if isinstance(o, Path):
        return str(o)
    return str(o)


def main():
    parser = argparse.ArgumentParser(description="ã‚ªãƒãƒãƒˆãƒšã‚’ã‚³ãƒå†…ã«é…ç½®ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    parser.add_argument("--onomatopoeia-dir", default="onomatopeias", help="ã‚ªãƒãƒãƒˆãƒšç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--mask-dir", default="onomatopeia_masks", help="ãƒã‚¹ã‚¯ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--background-dir", default="generated_double_backs", help="èƒŒæ™¯ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--output-dir", default="onomatopeia_datasets", help="åŸºæœ¬å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--dataset-name", type=str, default="panel_dataset01", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå")
    parser.add_argument("--target-images", type=int, default=100, help="ç”Ÿæˆã™ã‚‹ç”»åƒæ•°")
    parser.add_argument("--train-ratio", type=float, default=0.8, help="trainç”¨ã®æ¯”ç‡")
    
    args = parser.parse_args()
    
    # è¨­å®š
    CFG = {
        "SCALE_RANGE": (0.0001, 0.005),
        "SCALE_MODE": "lognormal",
        "SCALE_MEAN": 0.005623,
        "SCALE_STD": 0.014176,
        "SCALE_CLIP": (0.001, 0.030),  # ã‚ˆã‚Šåºƒã„ç¯„å›²ã«æ‹¡å¤§ï¼ˆ0.001ï½0.030 = 0.1%ï½3%ï¼‰
        "COUNT_STATS_FILE": "onomatopoeia_statistics.txt",
        "MAX_WIDTH_RATIO": 0.25,
        "MAX_HEIGHT_RATIO": 0.25,
        "MAX_ATTEMPTS": 50,
        "ONOMATOPOEIA_PER_PANEL": (1, 3),  # ãƒ‘ãƒãƒ«ã‚ãŸã‚Šã®ã‚ªãƒãƒãƒˆãƒšæ•°
        "PANEL_AREA_RATIO": 0.70,  # 0.85ã‹ã‚‰0.70ã«ç·©å’Œ
        "PANEL_MIN_AREA": 5000,    # 10000ã‹ã‚‰5000ã«ç·©å’Œ
        "SCALE_STATS": None,
    }
    
    # çµ±è¨ˆæƒ…å ±ã®èª­ã¿è¾¼ã¿
    stats_file = Path(CFG["COUNT_STATS_FILE"])
    if stats_file.exists():
        print(f"ğŸ“Š çµ±è¨ˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {stats_file}")
        CFG["SCALE_STATS"] = load_scale_stats(str(stats_file))
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    base_output_dir = args.output_dir
    dataset_name = args.dataset_name
    final_output_dir = Path(base_output_dir) / dataset_name
    
    os.makedirs(final_output_dir, exist_ok=True)
    
    print(f"\nğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {final_output_dir}")
    print(f"ğŸ¯ ç›®æ¨™ç”»åƒæ•°: {args.target_images}æš")
    
    # ã‚ªãƒãƒãƒˆãƒšã¨ãƒã‚¹ã‚¯ã®ãƒšã‚¢å–å¾—
    onomatopoeia_mask_pairs = []
    for ono_file in os.listdir(args.onomatopoeia_dir):
        if ono_file.endswith(('.png', '.jpg', '.jpeg')):
            ono_path = os.path.join(args.onomatopoeia_dir, ono_file)
            ono_stem = Path(ono_file).stem
            mask_file = f"{ono_stem}_mask.png"
            mask_path = os.path.join(args.mask_dir, mask_file)
            
            if os.path.exists(mask_path):
                onomatopoeia_mask_pairs.append((ono_path, mask_path))
    
    print(f"è¦‹ã¤ã‹ã£ãŸã‚ªãƒãƒãƒˆãƒšãƒšã‚¢: {len(onomatopoeia_mask_pairs)}å€‹")
    
    # èƒŒæ™¯ç”»åƒå–å¾—
    background_files = []
    for bg_file in os.listdir(args.background_dir):
        if bg_file.endswith(('.png', '.jpg', '.jpeg')):
            background_files.append(os.path.join(args.background_dir, bg_file))
    
    print(f"è¦‹ã¤ã‹ã£ãŸèƒŒæ™¯ç”»åƒ: {len(background_files)}å€‹")
    
    # train/valãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    for split in ["train", "val"]:
        (final_output_dir / split / "images").mkdir(parents=True, exist_ok=True)
        (final_output_dir / split / "masks").mkdir(parents=True, exist_ok=True)
    
    # ç”»åƒç”Ÿæˆ
    train_count = 0
    val_count = 0
    train_target = int(args.target_images * args.train_ratio)
    val_target = args.target_images - train_target
    
    train_number = 1
    val_number = 1
    bg_idx = 0
    
    print(f"\nğŸš€ ã‚ªãƒãƒãƒˆãƒšé…ç½®é–‹å§‹...")
    print(f"ç›®æ¨™: train {train_target}æš, val {val_target}æš")
    
    while train_count < train_target or val_count < val_target:
        if train_count >= train_target and val_count >= val_target:
            break
        
        # èƒŒæ™¯ç”»åƒã‚’å¾ªç’°ä½¿ç”¨
        bg_path = background_files[bg_idx % len(background_files)]
        bg_idx += 1
        
        try:
            # ã‚ªãƒãƒãƒˆãƒšé…ç½®
            result_img, result_mask, placements = composite_onomatopoeia_in_panels(
                bg_path, onomatopoeia_mask_pairs, CFG
            )
            
            if len(placements) == 0:
                continue  # é…ç½®ã§ããªã‹ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            
            # train/valã«æŒ¯ã‚Šåˆ†ã‘
            if train_count < train_target:
                split = "train"
                out_num = f"{train_number:03d}"
                train_number += 1
                train_count += 1
            elif val_count < val_target:
                split = "val"
                out_num = f"{val_number:03d}"
                val_number += 1
                val_count += 1
            else:
                continue  # å‰²ã‚Šå½“ã¦æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            
            # ä¿å­˜
            img_path = final_output_dir / split / "images" / f"{out_num}.png"
            mask_path = final_output_dir / split / "masks" / f"{out_num}.png"
            
            cv2.imwrite(str(img_path), result_img)
            cv2.imwrite(str(mask_path), result_mask)
            
        except Exception as e:
            print(f"âŒ åˆæˆå¤±æ•—: {Path(bg_path).name} - {str(e)}")
    
    print(f"\nâœ… ç”Ÿæˆå®Œäº†")
    print(f"train: {train_count}æš, val: {val_count}æš")
    print(f"åˆè¨ˆ: {train_count + val_count}æš")


if __name__ == "__main__":
    main()
