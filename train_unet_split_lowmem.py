"""
U-Net training (train/val/test    # ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ»ç·Šæ€¥åœæ­¢
    "ENABLE_EMERGENCY_STOP": True,   # False ã«ã™ã‚‹ã¨ç·Šæ€¥åœæ­¢ã‚’ç„¡åŠ¹åŒ–ï¼ˆè­¦å‘Šã®ã¿ï¼‰
    "EMERGENCY_GPU_THRESHOLD": 0.95,  # GPUä½¿ç”¨ç‡ã®ç·Šæ€¥åœæ­¢é–¾å€¤ (0.0-1.0)
    "EMERGENCY_RAM_THRESHOLD": 0.90,  # RAMä½¿ç”¨ç‡ã®ç·Šæ€¥åœæ­¢é–¾å€¤ (0.0-1.0)

    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    "USE_AMP": True,              # æ··åˆç²¾åº¦å­¦ç¿’ï¼ˆ-35ã€œ50% VRAMï¼‰
    "USE_GRAD_CHECKPOINT": True,  # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ-20ã€œ40% VRAMï¼‰
    "USE_CHANNELS_LAST": True,    # channels-last ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆé«˜é€ŸåŒ–ï¼‰

    # wandbrs) + periodic prediction dump
"""

import glob, random, time, os
from pathlib import Path
import psutil  # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ç”¨
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
import sys  # ç·Šæ€¥çµ‚äº†ç”¨

import numpy as np
import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.utils.checkpoint import checkpoint
from torchvision import transforms
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
import wandb

# --------------------------------------------------------------------
CFG = {
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    "ROOT":        Path("syn500-balloon-corner"),  # train/val ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ«ãƒ¼ãƒˆ
    "IMG_SIZE":    (512, 768),  # (height, width) = ç¸¦512 Ã— æ¨ª768 (ãƒ¡ãƒ¢ãƒªå‰Šæ¸› & ç¸¦é•·å¯¾å¿œ)

    # å­¦ç¿’
    "BATCH":       8,           # ãƒãƒƒãƒã‚µã‚¤ã‚º8ã«æˆ»ã™ï¼ˆç”»åƒã‚µã‚¤ã‚ºå‰Šæ¸›ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    "EPOCHS":      100,         # ãƒãƒƒãƒ8ãªã‚‰100ã‚¨ãƒãƒƒã‚¯
    "LR":          1e-4,        # ãƒãƒƒãƒã‚µã‚¤ã‚º8ã®æ¨™æº–å­¦ç¿’ç‡
    "PATIENCE":    10,
    "SEED":        42,

    # ãƒ¡ãƒ¢ãƒªç›£è¦–ãƒ»ç·Šæ€¥åœæ­¢
    "ENABLE_EMERGENCY_STOP": True,   # False ã«ã™ã‚‹ã¨ç·Šæ€¥åœæ­¢ã‚’ç„¡åŠ¹åŒ–ï¼ˆè­¦å‘Šã®ã¿ï¼‰
    "EMERGENCY_GPU_THRESHOLD": 0.95,  # GPUä½¿ç”¨ç‡ã®ç·Šæ€¥åœæ­¢é–¾å€¤ (0.0-1.0)
    "EMERGENCY_RAM_THRESHOLD": 0.90,  # RAMä½¿ç”¨ç‡ã®ç·Šæ€¥åœæ­¢é–¾å€¤ (0.0-1.0)

    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    "USE_AMP": True,              # æ··åˆç²¾åº¦å­¦ç¿’ï¼ˆ-35ã€œ50% VRAMï¼‰
    "USE_GRAD_CHECKPOINT": False,  # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆ-20ã€œ40% VRAMã€å­¦ç¿’æ™‚é–“+20ã€œ50%ï¼‰
    "USE_CHANNELS_LAST": True,    # channels-last ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆé«˜é€ŸåŒ–ï¼‹çœãƒ¡ãƒ¢ãƒªï¼‰

    # wandb
    "WANDB_PROJ":  "balloon-seg",
    "DATASET":     "syn500-corner", # ã¾ãŸã¯ "real" / "synreal" ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã£ã¦æ›¸ãæ›ãˆã‚‹
    "RUN_NAME":    "",

    "MODELS_DIR":  Path("models"),

    # äºˆæ¸¬ãƒã‚¹ã‚¯å‡ºåŠ›
    "SAVE_PRED_EVERY": 10,     # 5 â†’ 10 ã«å¤‰æ›´ï¼ˆé »åº¦ã‚’åŠæ¸›ï¼‰
    "PRED_SAMPLE_N":   2,      # 3 â†’ 2 ã«å‰Šæ¸›ï¼ˆç”»åƒæ•°å‰Šæ¸›ï¼‰
    "PRED_DIR":        "predictions",

    # å†é–‹ ckpt
    "RESUME":      "",
}
# --------------------------------------------------------------------

def seed_everything(seed):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True

def check_memory_safety(threshold_gpu=0.90, threshold_ram=0.85):
    """
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å±é™ºãƒ¬ãƒ™ãƒ«ãªã‚‰è­¦å‘Š
    
    Args:
        threshold_gpu: GPUä½¿ç”¨ç‡ã®å±é™ºé–¾å€¤ (0.0-1.0)
        threshold_ram: RAMä½¿ç”¨ç‡ã®å±é™ºé–¾å€¤ (0.0-1.0)
    
    Returns:
        (is_safe, warning_message)
    """
    warnings = []
    
    # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    if torch.cuda.is_available():
        mem_allocated = torch.cuda.memory_allocated() / 1e9
        mem_reserved = torch.cuda.memory_reserved() / 1e9
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        gpu_usage = mem_reserved / total_memory
        
        if gpu_usage > threshold_gpu:
            warnings.append(f"âš ï¸ GPUä½¿ç”¨ç‡ãŒå±é™º: {gpu_usage*100:.1f}% ({mem_reserved:.2f}GB/{total_memory:.2f}GB)")
    
    # RAM ãƒã‚§ãƒƒã‚¯
    ram = psutil.virtual_memory()
    ram_usage = ram.percent / 100.0
    
    if ram_usage > threshold_ram:
        warnings.append(f"âš ï¸ RAMä½¿ç”¨ç‡ãŒå±é™º: {ram_usage*100:.1f}% ({ram.used/1e9:.2f}GB/{ram.total/1e9:.2f}GB)")
    
    is_safe = len(warnings) == 0
    warning_message = "\n".join(warnings) if warnings else ""
    
    return is_safe, warning_message

def emergency_save_and_exit(model, cfg, epoch, reason):
    """
    ç·Šæ€¥æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦å®‰å…¨ã«çµ‚äº†
    
    Args:
        model: ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        cfg: è¨­å®šè¾æ›¸
        epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯
        reason: çµ‚äº†ç†ç”±
    """
    print("\n" + "="*60)
    print("ğŸš¨ ç·Šæ€¥åœæ­¢ãƒ¢ãƒ¼ãƒ‰")
    print("="*60)
    print(f"ç†ç”±: {reason}")
    print(f"ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯: {epoch}")
    
    # ç·Šæ€¥ä¿å­˜
    emergency_path = f"emergency_epoch{epoch}.pth"
    try:
        torch.save(model.state_dict(), emergency_path)
        print(f"âœ… ç·Šæ€¥ä¿å­˜å®Œäº†: {emergency_path}")
    except Exception as e:
        print(f"âŒ ç·Šæ€¥ä¿å­˜å¤±æ•—: {e}")
    
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    print("\nå®‰å…¨ã«çµ‚äº†ã—ã¾ã™...")
    print("="*60)
    sys.exit(0)

def print_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º"""
    print("\n" + "="*60)
    print("System Resource Check")
    print("="*60)
    
    # CPU/RAM
    try:
        import psutil
        ram = psutil.virtual_memory()
        print(f"RAM: {ram.used/1e9:.1f}GB / {ram.total/1e9:.1f}GB ({ram.percent}%)")
        if ram.percent > 85:
            print("âš ï¸  WARNING: RAM usage is high!")
    except ImportError:
        print("RAM: (psutil not installed)")
    
    # GPU
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB allocated")
        print(f"GPU Memory: {torch.cuda.memory_reserved()/1e9:.2f}GB reserved")
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"GPU Total: {total_mem:.1f}GB")
        
        # GPUæ¸©åº¦ï¼ˆè©¦è¡Œï¼‰
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            print(f"GPU Temperature: {temp}Â°C")
            if temp > 80:
                print("âš ï¸  WARNING: GPU temperature is high!")
            pynvml.nvmlShutdown()
        except:
            print("GPU Temperature: (unable to read)")
    else:
        print("GPU: Not available (using CPU)")
    
    print("="*60 + "\n")

# ---------------- Dataset ----------------
class BalloonDataset(Dataset):
    def __init__(self, img_dir, mask_dir, img_size):
        self.img_paths = sorted(glob.glob(str(img_dir / "*.png")))
        #jpgã«å¯¾å¿œ
        if not self.img_paths:
            self.img_paths = sorted(glob.glob(str(img_dir / "*.jpg")))
        if not self.img_paths:
            raise FileNotFoundError(f"No images found in {img_dir}")
        self.mask_dir  = mask_dir
        
        # img_size ãŒã‚¿ãƒ—ãƒ«ã®å ´åˆã¨intã®å ´åˆã«å¯¾å¿œ
        if isinstance(img_size, tuple):
            resize_size = img_size  # (H, W)
        else:
            resize_size = (img_size, img_size)  # æ­£æ–¹å½¢
        
        self.img_tf = transforms.Compose([
            transforms.Resize(resize_size),
            transforms.ToTensor(),
        ])
        self.mask_tf = transforms.Compose([
            transforms.Resize(resize_size, interpolation=Image.NEAREST),
            transforms.ToTensor(),
        ])

    def __len__(self): return len(self.img_paths)

    def __getitem__(self, idx):
        img_p = self.img_paths[idx]
        stem   = Path(img_p).stem
        mask_p = self.mask_dir / f"{stem}_mask.png"
        img  = self.img_tf(Image.open(img_p).convert("RGB"))
        mask = self.mask_tf(Image.open(mask_p).convert("L"))
        mask = (mask > .5).float()
        
        # channels-lastã¯ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã«é©ç”¨ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¸è¦
        # ï¼ˆDataLoaderãŒãƒãƒƒãƒã‚’ä½œã£ãŸå¾Œã€ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™å‰ã«é©ç”¨ï¼‰
        
        return img, mask, stem               # stem ã‚’è¿”ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã«åˆ©ç”¨

# --- é€£ç•ªã‚’è‡ªå‹•ã§æŒ¯ã‚‹ -------------------------------------------------
def next_version(models_dir, prefix):
    models_dir.mkdir(exist_ok=True)
    exist = sorted(models_dir.glob(f"{prefix}-*.pt"))
    if not exist: return "01"
    last = int(exist[-1].stem.split("-")[-1])
    return f"{last+1:02d}"

def collect_probs(model, loader, device, use_amp=False, use_channels_last=False):
    model.eval()
    probs, gts = [], []
    with torch.no_grad():
        for batch_idx, (x, y, _) in enumerate(loader):
            x = x.to(device)
            
            # channels-last é©ç”¨
            if use_channels_last:
                x = x.contiguous(memory_format=torch.channels_last)
            
            # AMPå¯¾å¿œã®æ¨è«–
            if use_amp:
                with autocast(dtype=torch.float16):
                    p = torch.sigmoid(model(x)).cpu().numpy()      # (B,1,H,W)
            else:
                p = torch.sigmoid(model(x)).cpu().numpy()
            
            probs.append(p.reshape(-1))                    # flatten
            gts.append(y.numpy().reshape(-1))
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del x, y, p
            if batch_idx % 3 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                
    probs = np.concatenate(probs)   # shape: [N_pixels]
    gts   = np.concatenate(gts)
    
    # å‡¦ç†å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
        
    return probs, gts

# -------------- U-Net --------------
# -------------- U-Net --------------
class DoubleConv(nn.Module):
    def __init__(self, in_c, out_c, use_checkpoint=False):
        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1)
        self.bn1   = nn.BatchNorm2d(out_c)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)
        self.bn2   = nn.BatchNorm2d(out_c)
        self.relu2 = nn.ReLU(inplace=True)

    def forward(self, x):
        if self.use_checkpoint and self.training:
            # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨ï¼ˆå­¦ç¿’æ™‚ã®ã¿ï¼‰
            def f1(x):
                return self.relu1(self.bn1(self.conv1(x)))
            def f2(x):
                return self.relu2(self.bn2(self.conv2(x)))
            x = checkpoint(f1, x, use_reentrant=False)
            x = checkpoint(f2, x, use_reentrant=False)
            return x
        else:
            # é€šå¸¸ã® forwardï¼ˆæ¨è«–æ™‚ã¾ãŸã¯ç„¡åŠ¹æ™‚ï¼‰
            x = self.relu1(self.bn1(self.conv1(x)))
            x = self.relu2(self.bn2(self.conv2(x)))
            return x

class UNet(nn.Module):
    def __init__(self, n_classes=1, chs=(64,128,256,512,1024), use_checkpoint=False):
        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.downs, in_c = nn.ModuleList(), 3
        for c in chs[:-1]:
            self.downs.append(DoubleConv(in_c, c, use_checkpoint)); in_c=c
        self.bottleneck = DoubleConv(chs[-2], chs[-1], use_checkpoint)
        self.ups_tr  = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i-1], 2,2)
                         for i in range(len(chs)-1,0,-1)])
        self.up_convs= nn.ModuleList([DoubleConv(chs[i], chs[i-1], use_checkpoint)
                         for i in range(len(chs)-1,0,-1)])
        self.out_conv= nn.Conv2d(chs[0], n_classes, 1)
        self.pool = nn.MaxPool2d(2)
    def forward(self,x):
        skips=[]
        for l in self.downs:
            x=l(x); skips.append(x); x=self.pool(x)
        x=self.bottleneck(x)
        for up,conv,sk in zip(self.ups_tr,self.up_convs,skips[::-1]):
            x=up(x); x=torch.cat([x,sk],1); x=conv(x)
        return self.out_conv(x)

# -------------- Loss --------------------
class DiceLoss(nn.Module):
    def forward(self,y_p,y_t,eps=1e-7):
        y_p=torch.sigmoid(y_p)
        inter=2*(y_p*y_t).sum((2,3))
        union=(y_p+y_t).sum((2,3))+eps
        return 1-(inter/union).mean()
class ComboLoss(nn.Module):
    def __init__(self,a=.5):
        super().__init__(); self.bce=nn.BCEWithLogitsLoss(); self.dice=DiceLoss(); self.a=a
    def forward(self,y_p,y_t):
        return self.a*self.bce(y_p,y_t)+(1-self.a)*self.dice(y_p,y_t)

# -------------- Train / Eval ------------
def train_epoch(model, loader, lossf, opt, dev, scaler, cfg=None, current_epoch=None):
    model.train()
    run = 0
    use_amp = cfg.get("USE_AMP", False) if cfg else False
    use_channels_last = cfg.get("USE_CHANNELS_LAST", False) if cfg else False
    
    for batch_idx, (x, y, _) in enumerate(tqdm(loader, desc="train", leave=False)):
        # ãƒ¡ãƒ¢ãƒªå®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰
        if batch_idx % 10 == 0:
            is_safe, warning = check_memory_safety(
                threshold_gpu=cfg.get("EMERGENCY_GPU_THRESHOLD", 0.95) if cfg else 0.95,
                threshold_ram=cfg.get("EMERGENCY_RAM_THRESHOLD", 0.90) if cfg else 0.90
            )
            if not is_safe:
                print(f"\n[Batch {batch_idx}] {warning}")
                # ç·Šæ€¥åœæ­¢ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿åœæ­¢
                if cfg and cfg.get("ENABLE_EMERGENCY_STOP", True) and current_epoch:
                    emergency_save_and_exit(model, cfg, current_epoch, warning)
                else:
                    print("  â†’ ç·Šæ€¥åœæ­¢ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰")
        
        x, y = x.to(dev, non_blocking=True), y.to(dev, non_blocking=True)
        
        # channels-last é©ç”¨ï¼ˆãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦ï¼‰
        if use_channels_last:
            x = x.contiguous(memory_format=torch.channels_last)
            y = y.contiguous(memory_format=torch.channels_last)
        
        opt.zero_grad(set_to_none=True)
        
        # æ··åˆç²¾åº¦å­¦ç¿’
        if use_amp:
            with autocast(dtype=torch.float16):
                logits = model(x)
                loss = lossf(logits, y)
            scaler.scale(loss).backward()
            scaler.step(opt)
            scaler.update()
        else:
            logits = model(x)
            loss = lossf(logits, y)
            loss.backward()
            opt.step()
        
        run += loss.item() * x.size(0)
        
        # ãƒãƒƒãƒã”ã¨ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾ï¼ˆã‚ˆã‚Šé »ç¹ã«ï¼‰
        del x, y, logits, loss
        
        # 5ãƒãƒƒãƒã”ã¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if batch_idx % 5 == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()  # Pythonã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    
    # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ãƒ¡ãƒ¢ãƒªå®Œå…¨ã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    return run / len(loader.dataset)

@torch.no_grad()
def eval_epoch(model, loader, dev, cfg=None, current_epoch=None):
    model.eval()
    dice = iou = 0
    use_amp = cfg.get("USE_AMP", False) if cfg else False
    use_channels_last = cfg.get("USE_CHANNELS_LAST", False) if cfg else False
    
    for batch_idx, (x, y, _) in enumerate(tqdm(loader, desc="eval ", leave=False)):
        # ãƒ¡ãƒ¢ãƒªå®‰å…¨ãƒã‚§ãƒƒã‚¯ï¼ˆ15ãƒãƒƒãƒã”ã¨ï¼‰
        if batch_idx % 15 == 0:
            is_safe, warning = check_memory_safety(
                threshold_gpu=cfg.get("EMERGENCY_GPU_THRESHOLD", 0.95) if cfg else 0.95,
                threshold_ram=cfg.get("EMERGENCY_RAM_THRESHOLD", 0.90) if cfg else 0.90
            )
            if not is_safe:
                print(f"\n[Eval Batch {batch_idx}] {warning}")
                # ç·Šæ€¥åœæ­¢ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿åœæ­¢
                if cfg and cfg.get("ENABLE_EMERGENCY_STOP", True) and current_epoch:
                    emergency_save_and_exit(model, cfg, current_epoch, warning)
                else:
                    print("  â†’ ç·Šæ€¥åœæ­¢ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆè©•ä¾¡ç¶™ç¶šï¼‰")
        
        x, y = x.to(dev, non_blocking=True), y.to(dev, non_blocking=True)
        
        # channels-last é©ç”¨ï¼ˆãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦ï¼‰
        if use_channels_last:
            x = x.contiguous(memory_format=torch.channels_last)
            y = y.contiguous(memory_format=torch.channels_last)
        
        # æ··åˆç²¾åº¦æ¨è«–
        if use_amp:
            with autocast(dtype=torch.float16):
                p = torch.sigmoid(model(x))
        else:
            p = torch.sigmoid(model(x))
        
        pb = (p > .5).float()
        inter = (pb * y).sum((2,3))
        union = (pb + y - pb * y).sum((2,3))
        
        # Diceè¨ˆç®—
        dice_den = (pb + y).sum((2,3)) + 1e-7
        dice += torch.where(dice_den > 1e-6, 2*inter/dice_den, torch.ones_like(dice_den)).mean().item() * x.size(0)
        
        # IoUè¨ˆç®—
        iou_den = union + 1e-7
        iou += torch.where(iou_den > 1e-6, inter/iou_den, torch.ones_like(iou_den)).mean().item() * x.size(0)
        
        # ãƒãƒƒãƒã”ã¨ã«ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del x, y, p, pb, inter, union, dice_den, iou_den
        
        # 5ãƒãƒƒãƒã”ã¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if batch_idx % 5 == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
    
    # è©•ä¾¡çµ‚äº†æ™‚ã«ãƒ¡ãƒ¢ãƒªå®Œå…¨ã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
        
    n = len(loader.dataset)
    return dice/n, iou/n

# -------------- Prediction dump ----------
@torch.no_grad()
def save_predictions(model, loader, cfg, epoch, run_dir, device):
    """val loader å…ˆé ­ã‹ã‚‰ PRED_SAMPLE_N æšæ¨è«–ã— PNG ä¿å­˜ & wandb ã¸ãƒ­ã‚°"""
    if epoch % cfg["SAVE_PRED_EVERY"] != 0: return
    model.eval()
    use_amp = cfg.get("USE_AMP", False)
    use_channels_last = cfg.get("USE_CHANNELS_LAST", False)

    pred_dir = run_dir / cfg["PRED_DIR"]
    pred_dir.mkdir(exist_ok=True)
    images_for_wandb = []

    cnt = 0
    for x, y, stem in loader:
        for i in range(len(x)):
            if cnt >= cfg["PRED_SAMPLE_N"]: break
            img = x[i:i+1].to(device)
            gt  = y[i]
            
            # channels-last é©ç”¨
            if use_channels_last:
                img = img.contiguous(memory_format=torch.channels_last)
            
            # AMPå¯¾å¿œã®æ¨è«–
            if use_amp:
                with autocast(dtype=torch.float16):
                    pred = torch.sigmoid(model(img))[0,0]          # (H,W)
            else:
                pred = torch.sigmoid(model(img))[0,0]
            
            pred_bin = (pred > 0.5).cpu().numpy()*255
            gt_np    = gt[0].cpu().numpy()*255

            # Save PNG
            out_path = pred_dir / f"pred_{epoch:03}_{stem[i]}.png"
            Image.fromarray(pred_bin.astype(np.uint8)).save(out_path)

            # wandb image (stack original, GT, pred)
            orig_np = (img[0].cpu().permute(1,2,0).numpy()*255).astype(np.uint8)
            
            # ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆè¡¨ç¤ºç”¨ãªã®ã§å“è³ªä½ä¸‹OKï¼‰
            h, w = orig_np.shape[:2]
            display_h, display_w = h // 2, w // 2  # åŠåˆ†ã®ã‚µã‚¤ã‚ºã«
            orig_small = Image.fromarray(orig_np).resize((display_w, display_h))
            gt_small = Image.fromarray(gt_np.astype(np.uint8)).resize((display_w, display_h))
            pred_small = Image.fromarray(pred_bin.astype(np.uint8)).resize((display_w, display_h))
            
            trio = np.concatenate([
                np.array(orig_small),
                np.stack([np.array(gt_small)]*3, 2),
                np.stack([np.array(pred_small)]*3, 2)
            ], axis=1)
            
            images_for_wandb.append(wandb.Image(trio,
                                    caption=f"ep{epoch:03}-{stem[i]}"))
            cnt += 1
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del img, gt, pred, pred_bin, gt_np, orig_np
            del orig_small, gt_small, pred_small, trio
            
        if cnt >= cfg["PRED_SAMPLE_N"]: break

    if images_for_wandb:
        wandb.log({f"pred_samples_epoch_{epoch}": images_for_wandb})
    
    # äºˆæ¸¬ä¿å­˜å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# -------------- Main ---------------------
def main():
    cfg=CFG; seed_everything(cfg["SEED"])
    dev="cuda" if torch.cuda.is_available() else "cpu"
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
    print_system_info()
    
    # ãƒ¡ãƒ¢ãƒªç›£è¦–è¨­å®šã‚’è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ›¡ï¸  è‡ªå‹•ãƒ¡ãƒ¢ãƒªç›£è¦–æ©Ÿèƒ½ æœ‰åŠ¹")
    print("="*60)
    if CFG.get("ENABLE_EMERGENCY_STOP", True):
        print("ç·Šæ€¥åœæ­¢: âœ… æœ‰åŠ¹")
        print("ç›£è¦–é–¾å€¤: (512Ã—512, ãƒãƒƒãƒ8ç”¨)")
        print(f"  - GPUä½¿ç”¨ç‡: {CFG.get('EMERGENCY_GPU_THRESHOLD', 0.95)*100:.0f}% ã§ç·Šæ€¥åœæ­¢")
        print(f"  - RAMä½¿ç”¨ç‡: {CFG.get('EMERGENCY_RAM_THRESHOLD', 0.90)*100:.0f}% ã§ç·Šæ€¥åœæ­¢")
    else:
        print("ç·Šæ€¥åœæ­¢: âš ï¸  ç„¡åŠ¹ï¼ˆè­¦å‘Šã®ã¿è¡¨ç¤ºï¼‰")
        print("  â€» ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã‚‚PCã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ã¾ã§å­¦ç¿’ç¶™ç¶šã—ã¾ã™")
    print("ãƒã‚§ãƒƒã‚¯é »åº¦:")
    print("  - Train: 10ãƒãƒƒãƒã”ã¨")
    print("  - Eval:  15ãƒãƒƒãƒã”ã¨")
    print("  - ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚: æ¯å›")
    print("ç·Šæ€¥åœæ­¢æ™‚ã®å‹•ä½œ:")
    print("  - ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ emergency_epochN.pth ã¨ã—ã¦ä¿å­˜")
    print("  - ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¦å®‰å…¨ã«çµ‚äº†")
    print("="*60 + "\n")
    
    prefix = f"{CFG['DATASET']}-unet"
    version = next_version(CFG["MODELS_DIR"], prefix)
    model_tag = f"{prefix}-{version}"          # synthetic-unet-01 ãªã©

    # wandb ã® run åãŒç©ºãªã‚‰ã“ã“ã§å…¥ã‚Œã‚‹
    if not CFG["RUN_NAME"]:
        CFG["RUN_NAME"] = model_tag

    wandb.init(project=CFG["WANDB_PROJ"], name=CFG["RUN_NAME"], config=CFG)
    run_dir = Path(wandb.run.dir)

    root=cfg["ROOT"]
    train_ds=BalloonDataset(root/"train/images",root/"train/masks",cfg["IMG_SIZE"])
    val_ds  =BalloonDataset(root/"val/images"  ,root/"val/masks"  ,cfg["IMG_SIZE"])
    #test_ds =BalloonDataset(root/"test/images" ,root/"test/masks" ,cfg["IMG_SIZE"])

    dl_tr=DataLoader(train_ds,batch_size=cfg["BATCH"],shuffle=True ,num_workers=0,pin_memory=False)
    dl_va=DataLoader(val_ds  ,batch_size=cfg["BATCH"],shuffle=False,num_workers=0,pin_memory=False)
    #dl_te=DataLoader(test_ds ,batch_size=cfg["BATCH"],shuffle=False,num_workers=4,pin_memory=True)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¯¾å¿œï¼‰
    model = UNet(use_checkpoint=cfg.get("USE_GRAD_CHECKPOINT", False)).to(dev)
    
    # channels-last ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
    if torch.cuda.is_available() and cfg.get("USE_CHANNELS_LAST", False):
        model = model.to(memory_format=torch.channels_last)
        print("âœ… Channels-last ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’é©ç”¨")
    
    if cfg["RESUME"]:
        model.load_state_dict(torch.load(cfg["RESUME"],map_location=dev)); print("Resumed")

    opt=torch.optim.AdamW(model.parameters(),lr=cfg["LR"])
    sched=torch.optim.lr_scheduler.CosineAnnealingLR(opt,T_max=cfg["EPOCHS"])
    lossf=ComboLoss()
    
    # æ··åˆç²¾åº¦å­¦ç¿’ç”¨ã®GradScalerï¼ˆæ–°ã—ã„APIï¼‰
    if cfg.get("USE_AMP", False):
        try:
            # PyTorch 2.0ä»¥é™ã®æ¨å¥¨API
            from torch.amp import GradScaler as NewGradScaler
            scaler = NewGradScaler('cuda')
        except ImportError:
            # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            scaler = GradScaler()
    else:
        scaler = None
    
    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šã®è¡¨ç¤º
    print("\n" + "="*60)
    print("âš¡ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š")
    print("="*60)
    print(f"æ··åˆç²¾åº¦ï¼ˆAMPï¼‰:         {'âœ… æœ‰åŠ¹' if cfg.get('USE_AMP', False) else 'âŒ ç„¡åŠ¹'}")
    print(f"å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ:     {'âœ… æœ‰åŠ¹' if cfg.get('USE_GRAD_CHECKPOINT', False) else 'âŒ ç„¡åŠ¹'}")
    print(f"Channels-last:          {'âœ… æœ‰åŠ¹' if cfg.get('USE_CHANNELS_LAST', False) else 'âŒ ç„¡åŠ¹'}")
    print("="*60 + "\n")

    best_iou=0; patience=0
    for ep in range(1,cfg["EPOCHS"]+1):
        t=time.time()
        
        # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªå®‰å…¨ãƒã‚§ãƒƒã‚¯
        is_safe, warning = check_memory_safety(
            threshold_gpu=cfg.get("EMERGENCY_GPU_THRESHOLD", 0.95) - 0.02,  # ã‚„ã‚„ç·©ã‚ã«
            threshold_ram=cfg.get("EMERGENCY_RAM_THRESHOLD", 0.90) - 0.02
        )
        if not is_safe:
            print(f"\n[Epoch {ep} Start] {warning}")
            # ç·Šæ€¥åœæ­¢ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿åœæ­¢
            if cfg.get("ENABLE_EMERGENCY_STOP", True):
                emergency_save_and_exit(model, cfg, ep, f"Epoché–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªä¸è¶³\n{warning}")
            else:
                print("  â†’ ç·Šæ€¥åœæ­¢ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰")
        
        # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
        if ep % 10 == 0 and torch.cuda.is_available():
            mem_allocated = torch.cuda.memory_allocated() / 1e9
            mem_reserved = torch.cuda.memory_reserved() / 1e9
            print(f"\n[Epoch {ep}] GPU Memory: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved")
        
        tr_loss=train_epoch(model, dl_tr, lossf, opt, dev, scaler, cfg, ep)
        va_dice,va_iou=eval_epoch(model, dl_va, dev, cfg, ep)
        sched.step()

        if ep % 5 == 0:
            probs, gts = collect_probs(model, dl_va, dev, cfg.get("USE_AMP", False), cfg.get("USE_CHANNELS_LAST", False))
            prec, rec, _ = precision_recall_curve(gts, probs)
            pr_auc = auc(rec, prec)
            
            # PR Curveã‚’ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
            sample_indices = np.linspace(0, len(rec)-1, min(200, len(rec)), dtype=int)
            rec_sampled = rec[sample_indices]
            prec_sampled = prec[sample_indices]
            
            wandb.log({"epoch": ep,
                    "val/pr_auc": pr_auc,
                    "val/pr_curve": wandb.plot.line(
                            wandb.Table(data=np.column_stack([rec_sampled, prec_sampled]),
                                        columns=["recall","precision"]),
                            "recall", "precision",
                            title=f"PR Curve ep{ep} (AUC={pr_auc:.3f})")})
            
            # PR Curveå¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del probs, gts, prec, rec, sample_indices, rec_sampled, prec_sampled
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        # --- äºˆæ¸¬ PNG ä¿å­˜ & wandb ç”»åƒãƒ­ã‚° ---
        save_predictions(model, dl_va, cfg, ep, run_dir, dev)
        
        # äºˆæ¸¬ä¿å­˜å¾Œã‚‚ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

        wandb.log({"epoch":ep,"loss":tr_loss,
                   "val_dice":va_dice,"val_iou":va_iou,
                   "lr":sched.get_last_lr()[0]})
        print(f"[{ep:03}] loss={tr_loss:.4f} dice={va_dice:.4f} iou={va_iou:.4f}  {time.time()-t:.1f}s")

        if va_iou>best_iou:
            best_iou,patience=va_iou,0
            ckpt_wandb = run_dir / f"best_ep{ep:03}_iou{va_iou:.4f}.pt"
            # torch.save(model.state_dict(), ckpt_wandb)
            # wandb.save(str(ckpt_wandb))
            torch.save(model.state_dict(), ckpt_wandb)

            # models/ ã«ã‚‚ã‚³ãƒ”ãƒ¼ï¼ˆå›ºå®šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
            ckpt_models = CFG["MODELS_DIR"] / f"{model_tag}.pt"
            torch.save(model.state_dict(), ckpt_models)
        else:
            patience+=1
            if patience>=cfg["PATIENCE"]:
                print("Early stopping."); break

    # # 1) ãƒ™ã‚¹ãƒˆé‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    # best_model_path = CFG["MODELS_DIR"] / f"{model_tag}.pt"
    # model.load_state_dict(torch.load(best_model_path, map_location=dev))

    # # 2) test ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    # test_ds = BalloonDataset(root/"test/images", root/"test/masks", cfg["IMG_SIZE"])
    # dl_te   = DataLoader(test_ds, batch_size=cfg["BATCH"], shuffle=False,
    #                      num_workers=4, pin_memory=True)

    # # 3) æ¨è«–ï¼†ãƒ­ã‚°
    # test_dice, test_iou = eval_epoch(model, dl_te, dev)
    # print(f"[TEST] Dice={test_dice:.4f}  IoU={test_iou:.4f}")
    # wandb.log({"test_dice": test_dice, "test_iou": test_iou})

if __name__=="__main__":
    main()
